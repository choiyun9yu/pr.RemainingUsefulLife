{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. CMAPSS 데이터 구조\n",
    "## 1-1. train_FD001\n",
    "- 여러 기기의 여러 센서 데이터와 RUL 정보를 포함한다.\n",
    "- 각 엔진은 고장까지의 여러 시간 단계에서 데이터를 기록한다.\n",
    "### 칼럼\n",
    "- First column: Engine ID\n",
    "- Second column: Time Cycle\n",
    "- 3rd-5th column: Operational Settings\n",
    "- 6th-26th column: Sensor measurements\n",
    "\n",
    "## 1-2. test_FD001\n",
    "- 고장 전 마지막까지의 데이터를 포함하며, 고장에 대한 정보는 포함하지 않고 있다.\n",
    "- 이 데이터를 사용해서 고장 시점을 예측해야 한다.\n",
    "\n",
    "## 1-3. RUL_FD001\n",
    "- test 데이터에 해당하는 각 엔진의 실제 남은 수명(RUL)을 포함하는 레이블이다"
   ],
   "id": "512ba5d515761cf2"
  },
  {
   "cell_type": "markdown",
   "id": "a43e1ea8-1c2f-497c-9e81-33b54ed3507f",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리\n",
    "\n",
    "## 2-1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615a854f-464d-4deb-a0f1-601abb45ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define filepath to read dataset\n",
    "data_path = './dataset/'\n",
    "\n",
    "# define column names\n",
    "columns = ['engine_id', 'time_in_cycles'] + \\\n",
    "          ['operational_setting_1', 'operational_setting_2', 'operational_setting_3'] + \\\n",
    "          [f'sensor_measurement_{i}' for i in range(1, 22)]\n",
    "\n",
    "train_df = pd.read_csv((data_path+'train_FD001.txt'), sep=r'\\s+', header=None, names=columns)\n",
    "test_df = pd.read_csv((data_path+'test_FD001.txt'), sep=r'\\s+', header=None, names=columns)\n",
    "rul_test_df = pd.read_csv((data_path+'RUL_FD001.txt'), sep=r'\\s+', header=None, names=['RUL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9403da6-5fe3-45df-b32c-27861333790c",
   "metadata": {},
   "source": [
    "## 2-2. RUL 라벨 추가(train data)\n",
    "- train 데이터는 고장 직전까지의 데이터를 가지고 있으므로, 각 엔진의 RUL 을 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443306b8-020e-489e-b54e-594c5563e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max cycle per engine (각 엔진의 마지막 cycle 을 고장 cycle 로 간주)\n",
    "max_cycle = train_df.groupby('engine_id')['time_in_cycles'].max()\n",
    "\n",
    "# Caculate RUL (최대 사이클 - 해당 칼럼 사이클)\n",
    "train_df = train_df.merge(max_cycle, on='engine_id', suffixes=('', '_max'))\n",
    "train_df['RUL'] = train_df['time_in_cycles_max'] - train_df['time_in_cycles']\n",
    "train_df.drop('time_in_cycles_max', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85823b-6d5e-4b17-acf7-86df0627fd5b",
   "metadata": {},
   "source": [
    "## 2-3. 특성 엔지니어링\n",
    "- 센서 데이터는 노이즈가 많을 수 있으므로, 불필요한 센서를 제거하거나 파생 변수를 생성하는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b7f534-cb7b-4944-9d87-0bbe45271f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센서 값이 일정한 (변동이 없는) 컬럼을 제거\n",
    "sensor_columns = [f'sensor_measurement_{i}' for i in range(1, 22)]\n",
    "constant_sensors = train_df[sensor_columns].std(axis=0) == 0\n",
    "train_df.drop(columns=constant_sensors.index[constant_sensors], axis=1, inplace=True)\n",
    "test_df.drop(columns=constant_sensors.index[constant_sensors], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b67a8-d4d4-442a-a2fd-017d9166fbbd",
   "metadata": {},
   "source": [
    "## 2-4. 데이터 정규화\n",
    "- 센서 데이터와 운영 조건들은 서로 다른 범위를 가질 수 있으므로, 이를 정규화해 주는 것이 일반적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8194c6f-caa6-4b7a-98ee-7c5710696f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "sensor_columns = [col for col in train_df.columns if 'sensor_measurement' in col]\n",
    "train_df[sensor_columns] = scaler.fit_transform(train_df[sensor_columns])\n",
    "test_df[sensor_columns] = scaler.transform(test_df[sensor_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d160bb92-cb2b-494c-ba25-659a02eaef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Z-Score Scaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# sensor_columns = [col for col in train_df.columns if 'sensor_measurement' in col]\n",
    "# train_df[sensor_columns] = scaler.fit_transform(train_df[sensor_columns])\n",
    "# test_df[sensor_columns] = scaler.transform(test_df[sensor_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f77138-d531-4c8a-9946-3bf1e6cf5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size for moving average\n",
    "window_size = 3  # You can adjust this value\n",
    "\n",
    "# Function to apply moving average to sensor columns\n",
    "def apply_moving_average(df):\n",
    "    sensor_columns = [col for col in df.columns if 'sensor_measurement' in col]\n",
    "    \n",
    "    # Group by engine_id and apply moving average\n",
    "    df_grouped = df.groupby('engine_id')[sensor_columns]\n",
    "    \n",
    "    # Apply rolling mean\n",
    "    df[sensor_columns] = df_grouped.rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply moving average to train_df and test_df\n",
    "train_df = apply_moving_average(train_df)\n",
    "test_df = apply_moving_average(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc997a4-ad98-4f3d-b1d3-0d56112a5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale sensor data\n",
    "scaler = StandardScaler()\n",
    "sensor_columns = [col for col in train_df.columns if 'sensor_measurement' in col]\n",
    "train_df[sensor_columns] = scaler.fit_transform(train_df[sensor_columns])\n",
    "test_df[sensor_columns] = scaler.transform(test_df[sensor_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dffb74-e38b-4154-99ac-6d908cc5e391",
   "metadata": {},
   "source": [
    "# 3. 학습 데이터 준비\n",
    "- LSTM, GRU 와 같은 시계열 모델을 사용하려면 각 엔진의 시계열 데이터를 고정된 기이의 시퀀스로 변환해야 한다.\n",
    "- 이를 위해 각 엔진의 데이터를 일정한 길이의 시퀀스로 잘라서 학습에 사용한다.\n",
    "- 예를 들어, 각 엔진의 시간 축 데이터를 50 단위의 시퀀스로 만들 수 있다.\n",
    "\n",
    "## 3-1. Create Sequence whith Sliding Window method\n",
    "- Sliding Window 기법을 사용하여 각 엔진의 시계열 데이터를 고정된 길이로 자르고, 해당 시퀀스를 학습에 사용할 수 있다.\n",
    "- 예를 들어, 각 엔진의 시계열 데이터를 50 사이클 단위로 자르는 코드를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2105085b-7ad1-4ac2-b573-2c6dfa704f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 시퀀스 길이 설정\n",
    "sequence_length = 50  # 50\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"주어진 데이터에서 시퀀스를 만드는 함수\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    engines = data['engine_id'].unique()\n",
    "    \n",
    "    for engine_id in engines:\n",
    "        engine_data = data[data['engine_id'] == engine_id]\n",
    "        engine_values = engine_data.drop(columns=['engine_id', 'RUL']).values\n",
    "        rul_values = engine_data['RUL'].values\n",
    "        \n",
    "        for i in range(len(engine_values) - sequence_length + 1):\n",
    "            sequences.append(engine_values[i: i + sequence_length])\n",
    "            targets.append(rul_values[i + sequence_length - 1])  # 시퀀스의 마지막 값이 목표 RUL\n",
    "            \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Train data에서 시퀀스와 타겟 생성\n",
    "train_sequences, train_targets = create_sequences(train_df, sequence_length)\n",
    "\n",
    "# 실제 RUL 값은 numpy 배열로 변환\n",
    "true_rul = rul_test_df.values.squeeze()  # RUL_FD001.csv에서 실제 RUL 값 (예: rul_test_df = pd.read_csv('RUL_FD001.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be137957-a24d-464c-b7dc-a362d41e298d",
   "metadata": {},
   "source": [
    "## 3-2. 테스트 데이터에 대한 시퀀스 생성\n",
    "- 테스트 데이터는 각 엔진의 마지막 시점까지의 데이터를 포함하고 있다.\n",
    "- 테스트 데이터를 처리할 때는 각 엔진의 마지막 sequence_length 만큼의 데이터를 사용하여 예측해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d3b37e9-1c79-44fb-9f7b-573d545e57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_test_sequences(data, sequence_length):\n",
    "#     \"\"\"테스트 데이터를 위해 마지막 sequence_length만큼 시퀀스를 만드는 함수\"\"\"\n",
    "#     sequences = []\n",
    "    \n",
    "#     engines = data['engine_id'].unique()\n",
    "    \n",
    "#     for engine_id in engines:\n",
    "#         engine_data = data[data['engine_id'] == engine_id]\n",
    "#         engine_values = engine_data.drop(columns=['engine_id']).values\n",
    "        \n",
    "#         # 시퀀스 길이가 sequence_length보다 짧으면 제외\n",
    "#         if len(engine_values) >= sequence_length:\n",
    "#             sequences.append(engine_values[-sequence_length:])  # 마지막 sequence_length만큼 사용\n",
    "    \n",
    "#     return np.array(sequences)\n",
    "\n",
    "# # Test data에서 시퀀스 생성\n",
    "# test_sequences = create_test_sequences(test_df, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371bee87-35f3-43f9-9979-5d9cd4cfd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_test_sequences(data, sequence_length):\n",
    "    \"\"\"테스트 데이터를 위해 마지막 sequence_length만큼 시퀀스를 만드는 함수\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    engines = data['engine_id'].unique()\n",
    "    \n",
    "    for engine_id in engines:\n",
    "        engine_data = data[data['engine_id'] == engine_id]\n",
    "        engine_values = engine_data.drop(columns=['engine_id']).values\n",
    "        \n",
    "        # 시퀀스 길이가 sequence_length보다 짧으면 앞에 0으로 패딩\n",
    "        if len(engine_values) < sequence_length:\n",
    "            padding = np.zeros((sequence_length - len(engine_values), engine_values.shape[1]))\n",
    "            engine_values = np.vstack((padding, engine_values))\n",
    "        \n",
    "        sequences.append(engine_values[-sequence_length:])  # 마지막 sequence_length 만큼 사용\n",
    "    \n",
    "    return np.array(sequences)\n",
    "\n",
    "# Test data에서 시퀀스 생성\n",
    "test_sequences = create_test_sequences(test_df, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a94224-e474-4878-b521-347a309fc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "# def create_test_sequences(data, sequence_length):\n",
    "#     \"\"\"테스트 데이터를 위해 마지막 sequence_length만큼 시퀀스를 만드는 함수\"\"\"\n",
    "#     sequences = []\n",
    "#     sequence_lengths = []\n",
    "    \n",
    "#     engines = data['engine_id'].unique()\n",
    "    \n",
    "#     for engine_id in engines:\n",
    "#         engine_data = data[data['engine_id'] == engine_id]\n",
    "#         engine_values = engine_data.drop(columns=['engine_id']).values\n",
    "        \n",
    "#         # 시퀀스 길이가 sequence_length보다 짧으면 앞에 0으로 패딩\n",
    "#         if len(engine_values) < sequence_length:\n",
    "#             padding = np.zeros((sequence_length - len(engine_values), engine_values.shape[1]))\n",
    "#             engine_values = np.vstack((padding, engine_values))\n",
    "        \n",
    "#         sequences.append(engine_values[-sequence_length:])  # 마지막 sequence_length만큼 사용\n",
    "#         sequence_lengths.append(len(engine_data))  # 원래 시퀀스 길이 저장\n",
    "    \n",
    "#     return torch.tensor(sequences, dtype=torch.float32), sequence_lengths\n",
    "\n",
    "# # Test data에서 시퀀스 생성\n",
    "# test_sequences, sequence_lengths = create_test_sequences(test_df, sequence_length)\n",
    "\n",
    "# # PackedSequence로 변환\n",
    "# packed_test_sequences = rnn_utils.pack_padded_sequence(test_sequences, sequence_lengths, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d0117-0ff3-49d5-b0b2-a21bc243d8fd",
   "metadata": {},
   "source": [
    "## 3-3. 텐서 변환\n",
    "- PyTorch 를 사용하기 위해서는 데이터를 텐서로 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa64c357-3e23-4d6d-83a6-f1d32f376a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 데이터를 텐서로 변환\n",
    "train_sequences_tensor = torch.tensor(train_sequences, dtype=torch.float32)\n",
    "train_targets_tensor = torch.tensor(train_targets, dtype=torch.float32)\n",
    "test_sequences_tensor = torch.tensor(test_sequences, dtype=torch.float32)\n",
    "test_targets_tensor = torch.tensor(true_rul, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc4cf3-52c7-4d0e-a36a-888177c521c1",
   "metadata": {},
   "source": [
    "# 4. PyTorch LSTM 모델 설계\n",
    "- LSTM Layer: 여러 개의 LSTM 레이어를 쌓은 구조이며, 각 타임 스텝의 출력을 계산한다.\n",
    "- FC Layer: 마지막 LSTM 의 출력에서 가장 마지막 타임스텝의 출력을 가져와서 Fully Connected Layer 로 RUL 을 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa8e2cd-bc29-43a4-9809-7d2db8620751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # RUL을 예측하기 위한 출력 레이어\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM에 입력: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # hidden state 초기화\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # cell state 초기화\n",
    "        \n",
    "        # LSTM의 출력: (batch_size, sequence_length, hidden_size), (hn, cn)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 최종 타임스텝의 출력만 사용\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer를 통해 RUL 예측\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    # 최적의 가중치 저장하는 함수\n",
    "    def save_best_state_dict(self):\n",
    "        self.best_state_dict = self.state_dict()\n",
    "        return self.best_state_dict\n",
    "\n",
    "    # 최적의 가중치를 반환하는 함수\n",
    "    def get_best_state_dict(self):\n",
    "        return self.best_state_dict\n",
    "\n",
    "# 스코어 함수 선언 asymmetric_scoring 함수는 조기 예측과 늦은 예측에 대해 서로 다른 가중치를 적용하여 스코어를 계산한다. (조기 예측 가중치 a1, 늦은 예측 가중치 a2)\n",
    "def asymmetric_scoring(y_true, y_pred, a1=10, a2=13):\n",
    "    \"\"\"\n",
    "    비대칭 스코어링 함수\n",
    "    y_true: 실제 RUL 값 (numpy array)\n",
    "    y_pred: 예측된 RUL 값 (numpy array)\n",
    "    a1: 조기 예측에 대한 가중치\n",
    "    a2: 늦은 예측에 대한 가중치\n",
    "    \"\"\"\n",
    "    errors = y_pred - y_true\n",
    "    scores = np.where(errors < 0, np.exp(-errors / a1) - 1, np.exp(errors / a2) - 1)\n",
    "    return np.sum(scores)\n",
    "\n",
    "# evaluate_algorithm 함수는 여러 UUT 에 대해 총 스코어를 계산한다.\n",
    "def evaluate_algorithm(y_true_all, y_pred_all, a1=10, a2=13):\n",
    "    \"\"\"\n",
    "    알고리즘 평가 함수\n",
    "    y_true_all: 실제 RUL 값 리스트 (각 UUT별 numpy array)\n",
    "    y_pred_all: 예측된 RUL 값 리스트 (각 UUT별 numpy array)\n",
    "    a1: 조기 예측에 대한 가중치\n",
    "    a2: 늦은 예측에 대한 가중치\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "\n",
    "    for y_true, y_pred in zip(y_true_all, y_pred_all):\n",
    "        score = asymmetric_scoring(y_true, y_pred, a1, a2)\n",
    "        total_score += score\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5502647-9aa3-43c1-882d-6e6fdb0dab61",
   "metadata": {},
   "source": [
    "## 4-1. 모델 초기화\n",
    "- LSTM 모델을 초기화하고, 입력 크기, LSTM 의 hidden size, LSTM 레이어 수 등ㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be488b44-50c9-467a-b8f9-c3cba19142da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_size = train_sequences.shape[2]  # 센서/피처 수\n",
    "hidden_size = 50  # LSTM hidden state 크기 (높을 수로 모델 복잡해짐)\n",
    "num_layers = 2  # LSTM 레이어 수\n",
    "learning_rate = 0.0005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, hidden_size, num_layers).to(device)  # 모델을 GPU로 이동\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "set_seed(369)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.MSELoss()  # RUL 예측이므로 MSELoss 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam Optimizer 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6bbd7-96c8-4e2c-85a4-b97a881a0ad5",
   "metadata": {},
   "source": [
    "# 5. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "876f09b1-85fe-4cf8-b88b-310ddcda97fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 12976.6323, Val Loss: 7998.0570\n",
      "Saved Best Model\n",
      "Epoch [2/200], Loss: 12532.1970, Val Loss: 7628.6327\n",
      "Saved Best Model\n",
      "Epoch [3/200], Loss: 12147.8591, Val Loss: 7383.7244\n",
      "Saved Best Model\n",
      "Epoch [4/200], Loss: 11904.7468, Val Loss: 7222.2206\n",
      "Saved Best Model\n",
      "Epoch [5/200], Loss: 11718.8541, Val Loss: 7085.2933\n",
      "Saved Best Model\n",
      "Epoch [6/200], Loss: 11553.5585, Val Loss: 6957.7430\n",
      "Saved Best Model\n",
      "Epoch [7/200], Loss: 11398.0580, Val Loss: 6837.7968\n",
      "Saved Best Model\n",
      "Epoch [8/200], Loss: 11249.9520, Val Loss: 6722.4985\n",
      "Saved Best Model\n",
      "Epoch [9/200], Loss: 11106.7126, Val Loss: 6611.1820\n",
      "Saved Best Model\n",
      "Epoch [10/200], Loss: 10967.4377, Val Loss: 6503.8100\n",
      "Saved Best Model\n",
      "Epoch [11/200], Loss: 10832.6871, Val Loss: 6398.2556\n",
      "Saved Best Model\n",
      "Epoch [12/200], Loss: 10700.3567, Val Loss: 6295.7413\n",
      "Saved Best Model\n",
      "Epoch [13/200], Loss: 10570.7031, Val Loss: 6196.7166\n",
      "Saved Best Model\n",
      "Epoch [14/200], Loss: 10444.7347, Val Loss: 6098.7012\n",
      "Saved Best Model\n",
      "Epoch [15/200], Loss: 10320.7609, Val Loss: 6002.9103\n",
      "Saved Best Model\n",
      "Epoch [16/200], Loss: 10198.5602, Val Loss: 5910.3231\n",
      "Saved Best Model\n",
      "Epoch [17/200], Loss: 10079.7215, Val Loss: 5818.1653\n",
      "Saved Best Model\n",
      "Epoch [18/200], Loss: 9961.7281, Val Loss: 5729.3700\n",
      "Saved Best Model\n",
      "Epoch [19/200], Loss: 9846.9158, Val Loss: 5641.1369\n",
      "Saved Best Model\n",
      "Epoch [20/200], Loss: 9733.0917, Val Loss: 5555.5787\n",
      "Saved Best Model\n",
      "Epoch [21/200], Loss: 9621.5480, Val Loss: 5470.9902\n",
      "Saved Best Model\n",
      "Epoch [22/200], Loss: 9511.5489, Val Loss: 5388.6170\n",
      "Saved Best Model\n",
      "Epoch [23/200], Loss: 9403.8759, Val Loss: 5307.0310\n",
      "Saved Best Model\n",
      "Epoch [24/200], Loss: 9296.9857, Val Loss: 5227.4023\n",
      "Saved Best Model\n",
      "Epoch [25/200], Loss: 9191.7741, Val Loss: 5149.4517\n",
      "Saved Best Model\n",
      "Epoch [26/200], Loss: 9088.3667, Val Loss: 5073.1329\n",
      "Saved Best Model\n",
      "Epoch [27/200], Loss: 8987.0836, Val Loss: 4997.7547\n",
      "Saved Best Model\n",
      "Epoch [28/200], Loss: 8886.8702, Val Loss: 4924.0984\n",
      "Saved Best Model\n",
      "Epoch [29/200], Loss: 8788.6869, Val Loss: 4851.5055\n",
      "Saved Best Model\n",
      "Epoch [30/200], Loss: 8691.4744, Val Loss: 4781.1032\n",
      "Saved Best Model\n",
      "Epoch [31/200], Loss: 8596.2313, Val Loss: 4711.6808\n",
      "Saved Best Model\n",
      "Epoch [32/200], Loss: 8502.4967, Val Loss: 4643.2796\n",
      "Saved Best Model\n",
      "Epoch [33/200], Loss: 8410.0719, Val Loss: 4576.3320\n",
      "Saved Best Model\n",
      "Epoch [34/200], Loss: 8319.0927, Val Loss: 4510.8256\n",
      "Saved Best Model\n",
      "Epoch [35/200], Loss: 8229.6497, Val Loss: 4446.4631\n",
      "Saved Best Model\n",
      "Epoch [36/200], Loss: 8141.2260, Val Loss: 4384.0366\n",
      "Saved Best Model\n",
      "Epoch [37/200], Loss: 8054.6274, Val Loss: 4322.4490\n",
      "Saved Best Model\n",
      "Epoch [38/200], Loss: 7969.3447, Val Loss: 4261.8482\n",
      "Saved Best Model\n",
      "Epoch [39/200], Loss: 7885.3694, Val Loss: 4202.1855\n",
      "Saved Best Model\n",
      "Epoch [40/200], Loss: 7802.4688, Val Loss: 4144.4159\n",
      "Saved Best Model\n",
      "Epoch [41/200], Loss: 7719.8041, Val Loss: 4087.5805\n",
      "Saved Best Model\n",
      "Epoch [42/200], Loss: 7634.7708, Val Loss: 4010.4084\n",
      "Saved Best Model\n",
      "Epoch [43/200], Loss: 7532.9835, Val Loss: 3954.9647\n",
      "Saved Best Model\n",
      "Epoch [44/200], Loss: 7429.2303, Val Loss: 3849.7077\n",
      "Saved Best Model\n",
      "Epoch [45/200], Loss: 7320.5084, Val Loss: 3786.1447\n",
      "Saved Best Model\n",
      "Epoch [46/200], Loss: 7232.2310, Val Loss: 3727.2699\n",
      "Saved Best Model\n",
      "Epoch [47/200], Loss: 7147.9627, Val Loss: 3667.3341\n",
      "Saved Best Model\n",
      "Epoch [48/200], Loss: 7065.6520, Val Loss: 3610.8372\n",
      "Saved Best Model\n",
      "Epoch [49/200], Loss: 6984.5872, Val Loss: 3561.1553\n",
      "Saved Best Model\n",
      "Epoch [50/200], Loss: 6904.3380, Val Loss: 3492.4917\n",
      "Saved Best Model\n",
      "Epoch [51/200], Loss: 6825.4869, Val Loss: 3441.2663\n",
      "Saved Best Model\n",
      "Epoch [52/200], Loss: 6746.7878, Val Loss: 3384.4596\n",
      "Saved Best Model\n",
      "Epoch [53/200], Loss: 6670.6003, Val Loss: 3324.4531\n",
      "Saved Best Model\n",
      "Epoch [54/200], Loss: 6594.5445, Val Loss: 3275.6653\n",
      "Saved Best Model\n",
      "Epoch [55/200], Loss: 6519.0712, Val Loss: 3219.6034\n",
      "Saved Best Model\n",
      "Epoch [56/200], Loss: 6445.0499, Val Loss: 3174.5596\n",
      "Saved Best Model\n",
      "Epoch [57/200], Loss: 6371.6199, Val Loss: 3122.1619\n",
      "Saved Best Model\n",
      "Epoch [58/200], Loss: 6299.1539, Val Loss: 3070.1343\n",
      "Saved Best Model\n",
      "Epoch [59/200], Loss: 6228.3784, Val Loss: 3024.3623\n",
      "Saved Best Model\n",
      "Epoch [60/200], Loss: 6155.6068, Val Loss: 2963.5077\n",
      "Saved Best Model\n",
      "Epoch [61/200], Loss: 6083.7951, Val Loss: 2913.9933\n",
      "Saved Best Model\n",
      "Epoch [62/200], Loss: 6014.6207, Val Loss: 2868.5059\n",
      "Saved Best Model\n",
      "Epoch [63/200], Loss: 5945.2018, Val Loss: 2816.9314\n",
      "Saved Best Model\n",
      "Epoch [64/200], Loss: 5876.6405, Val Loss: 2771.4884\n",
      "Saved Best Model\n",
      "Epoch [65/200], Loss: 5809.3206, Val Loss: 2728.0461\n",
      "Saved Best Model\n",
      "Epoch [66/200], Loss: 5741.9261, Val Loss: 2679.8557\n",
      "Saved Best Model\n",
      "Epoch [67/200], Loss: 5679.3663, Val Loss: 2634.2680\n",
      "Saved Best Model\n",
      "Epoch [68/200], Loss: 5611.9193, Val Loss: 2596.1015\n",
      "Saved Best Model\n",
      "Epoch [69/200], Loss: 5549.3135, Val Loss: 2547.6353\n",
      "Saved Best Model\n",
      "Epoch [70/200], Loss: 5486.0259, Val Loss: 2506.6655\n",
      "Saved Best Model\n",
      "Epoch [71/200], Loss: 5421.7407, Val Loss: 2463.8534\n",
      "Saved Best Model\n",
      "Epoch [72/200], Loss: 5360.5413, Val Loss: 2421.7994\n",
      "Saved Best Model\n",
      "Epoch [73/200], Loss: 5298.1895, Val Loss: 2383.8274\n",
      "Saved Best Model\n",
      "Epoch [74/200], Loss: 5241.5465, Val Loss: 2340.0429\n",
      "Saved Best Model\n",
      "Epoch [75/200], Loss: 5178.9675, Val Loss: 2302.2383\n",
      "Saved Best Model\n",
      "Epoch [76/200], Loss: 5120.0092, Val Loss: 2262.6801\n",
      "Saved Best Model\n",
      "Epoch [77/200], Loss: 5059.5914, Val Loss: 2224.6828\n",
      "Saved Best Model\n",
      "Epoch [78/200], Loss: 5002.9053, Val Loss: 2184.7362\n",
      "Saved Best Model\n",
      "Epoch [79/200], Loss: 4945.8416, Val Loss: 2147.4335\n",
      "Saved Best Model\n",
      "Epoch [80/200], Loss: 4888.1166, Val Loss: 2110.3196\n",
      "Saved Best Model\n",
      "Epoch [81/200], Loss: 4833.1334, Val Loss: 2075.9674\n",
      "Saved Best Model\n",
      "Epoch [82/200], Loss: 4777.8238, Val Loss: 2038.7304\n",
      "Saved Best Model\n",
      "Epoch [83/200], Loss: 4723.3450, Val Loss: 2002.7920\n",
      "Saved Best Model\n",
      "Epoch [84/200], Loss: 4668.9666, Val Loss: 1969.8555\n",
      "Saved Best Model\n",
      "Epoch [85/200], Loss: 4617.9669, Val Loss: 1935.6863\n",
      "Saved Best Model\n",
      "Epoch [86/200], Loss: 4564.4634, Val Loss: 1906.1367\n",
      "Saved Best Model\n",
      "Epoch [87/200], Loss: 4513.7045, Val Loss: 1867.3441\n",
      "Saved Best Model\n",
      "Epoch [88/200], Loss: 4461.4371, Val Loss: 1834.3470\n",
      "Saved Best Model\n",
      "Epoch [89/200], Loss: 4411.5645, Val Loss: 1810.3523\n",
      "Saved Best Model\n",
      "Epoch [90/200], Loss: 4370.4153, Val Loss: 1774.9836\n",
      "Saved Best Model\n",
      "Epoch [91/200], Loss: 4313.9053, Val Loss: 1739.1044\n",
      "Saved Best Model\n",
      "Epoch [92/200], Loss: 4264.7146, Val Loss: 1712.2560\n",
      "Saved Best Model\n",
      "Epoch [93/200], Loss: 4220.0909, Val Loss: 1703.3716\n",
      "Saved Best Model\n",
      "Epoch [94/200], Loss: 4174.4517, Val Loss: 1653.8283\n",
      "Saved Best Model\n",
      "Epoch [95/200], Loss: 4120.1452, Val Loss: 1620.5866\n",
      "Saved Best Model\n",
      "Epoch [96/200], Loss: 4073.3790, Val Loss: 1592.2936\n",
      "Saved Best Model\n",
      "Epoch [97/200], Loss: 4026.2192, Val Loss: 1562.7349\n",
      "Saved Best Model\n",
      "Epoch [98/200], Loss: 3982.8380, Val Loss: 1535.6815\n",
      "Saved Best Model\n",
      "Epoch [99/200], Loss: 3935.5522, Val Loss: 1508.4193\n",
      "Saved Best Model\n",
      "Epoch [100/200], Loss: 3889.4142, Val Loss: 1479.7197\n",
      "Saved Best Model\n",
      "Epoch [101/200], Loss: 3845.7935, Val Loss: 1454.7539\n",
      "Saved Best Model\n",
      "Epoch [102/200], Loss: 3801.8171, Val Loss: 1432.3297\n",
      "Saved Best Model\n",
      "Epoch [103/200], Loss: 3758.7226, Val Loss: 1400.9276\n",
      "Saved Best Model\n",
      "Epoch [104/200], Loss: 3716.3453, Val Loss: 1376.1434\n",
      "Saved Best Model\n",
      "Epoch [105/200], Loss: 3673.1514, Val Loss: 1352.0626\n",
      "Saved Best Model\n",
      "Epoch [106/200], Loss: 3635.5757, Val Loss: 1333.7291\n",
      "Saved Best Model\n",
      "Epoch [107/200], Loss: 3592.0330, Val Loss: 1300.8530\n",
      "Saved Best Model\n",
      "Epoch [108/200], Loss: 3549.8913, Val Loss: 1293.0871\n",
      "Saved Best Model\n",
      "Epoch [109/200], Loss: 3510.0594, Val Loss: 1255.2361\n",
      "Saved Best Model\n",
      "Epoch [110/200], Loss: 3473.4969, Val Loss: 1236.9206\n",
      "Saved Best Model\n",
      "Epoch [111/200], Loss: 3435.0333, Val Loss: 1208.2698\n",
      "Saved Best Model\n",
      "Epoch [112/200], Loss: 3394.2680, Val Loss: 1187.9769\n",
      "Saved Best Model\n",
      "Epoch [113/200], Loss: 3356.0112, Val Loss: 1162.6010\n",
      "Saved Best Model\n",
      "Epoch [114/200], Loss: 3315.0462, Val Loss: 1145.7313\n",
      "Saved Best Model\n",
      "Epoch [115/200], Loss: 3280.0481, Val Loss: 1126.8403\n",
      "Saved Best Model\n",
      "Epoch [116/200], Loss: 3242.7994, Val Loss: 1108.4378\n",
      "Saved Best Model\n",
      "Epoch [117/200], Loss: 3210.7030, Val Loss: 1089.4070\n",
      "Saved Best Model\n",
      "Epoch [118/200], Loss: 3171.6749, Val Loss: 1057.8574\n",
      "Saved Best Model\n",
      "Epoch [119/200], Loss: 3132.8260, Val Loss: 1037.6086\n",
      "Saved Best Model\n",
      "Epoch [120/200], Loss: 3096.9162, Val Loss: 1021.6739\n",
      "Saved Best Model\n",
      "Epoch [121/200], Loss: 3059.4241, Val Loss: 998.2316\n",
      "Saved Best Model\n",
      "Epoch [122/200], Loss: 3024.1657, Val Loss: 978.3968\n",
      "Saved Best Model\n",
      "Epoch [123/200], Loss: 2993.6690, Val Loss: 966.4145\n",
      "Saved Best Model\n",
      "Epoch [124/200], Loss: 2959.4625, Val Loss: 941.7212\n",
      "Saved Best Model\n",
      "Epoch [125/200], Loss: 2943.1343, Val Loss: 937.8791\n",
      "Saved Best Model\n",
      "Epoch [126/200], Loss: 2940.6719, Val Loss: 933.0985\n",
      "Saved Best Model\n",
      "Epoch [127/200], Loss: 2876.9481, Val Loss: 893.7933\n",
      "Saved Best Model\n",
      "Epoch [128/200], Loss: 2833.9447, Val Loss: 873.6193\n",
      "Saved Best Model\n",
      "Epoch [129/200], Loss: 2801.0245, Val Loss: 853.8286\n",
      "Saved Best Model\n",
      "Epoch [130/200], Loss: 2766.8546, Val Loss: 840.7896\n",
      "Saved Best Model\n",
      "Epoch [131/200], Loss: 2738.2635, Val Loss: 835.1150\n",
      "Saved Best Model\n",
      "Epoch [132/200], Loss: 2725.9630, Val Loss: 839.1656\n",
      "Epoch [133/200], Loss: 2698.1396, Val Loss: 798.5918\n",
      "Saved Best Model\n",
      "Epoch [134/200], Loss: 2648.4523, Val Loss: 776.2384\n",
      "Saved Best Model\n",
      "Epoch [135/200], Loss: 2614.8806, Val Loss: 756.3801\n",
      "Saved Best Model\n",
      "Epoch [136/200], Loss: 2583.8446, Val Loss: 744.4698\n",
      "Saved Best Model\n",
      "Epoch [137/200], Loss: 2552.1365, Val Loss: 727.9448\n",
      "Saved Best Model\n",
      "Epoch [138/200], Loss: 2522.3777, Val Loss: 710.8729\n",
      "Saved Best Model\n",
      "Epoch [139/200], Loss: 2493.7232, Val Loss: 699.1546\n",
      "Saved Best Model\n",
      "Epoch [140/200], Loss: 2470.1111, Val Loss: 694.9762\n",
      "Saved Best Model\n",
      "Epoch [141/200], Loss: 2437.3832, Val Loss: 672.0194\n",
      "Saved Best Model\n",
      "Epoch [142/200], Loss: 2410.8745, Val Loss: 660.3890\n",
      "Saved Best Model\n",
      "Epoch [143/200], Loss: 2381.8489, Val Loss: 643.0605\n",
      "Saved Best Model\n",
      "Epoch [144/200], Loss: 2354.1648, Val Loss: 631.9238\n",
      "Saved Best Model\n",
      "Epoch [145/200], Loss: 2331.2022, Val Loss: 615.9530\n",
      "Saved Best Model\n",
      "Epoch [146/200], Loss: 2299.4146, Val Loss: 596.7715\n",
      "Saved Best Model\n",
      "Epoch [147/200], Loss: 2279.9706, Val Loss: 596.2666\n",
      "Saved Best Model\n",
      "Epoch [148/200], Loss: 2253.1577, Val Loss: 578.1929\n",
      "Saved Best Model\n",
      "Epoch [149/200], Loss: 2255.5738, Val Loss: 608.0342\n",
      "Epoch [150/200], Loss: 2216.7583, Val Loss: 564.8959\n",
      "Saved Best Model\n",
      "Epoch [151/200], Loss: 2181.2805, Val Loss: 542.1383\n",
      "Saved Best Model\n",
      "Epoch [152/200], Loss: 2152.7389, Val Loss: 531.3936\n",
      "Saved Best Model\n",
      "Epoch [153/200], Loss: 2124.8980, Val Loss: 513.5692\n",
      "Saved Best Model\n",
      "Epoch [154/200], Loss: 2100.8914, Val Loss: 507.9778\n",
      "Saved Best Model\n",
      "Epoch [155/200], Loss: 2071.6789, Val Loss: 490.7738\n",
      "Saved Best Model\n",
      "Epoch [156/200], Loss: 2046.9275, Val Loss: 476.5543\n",
      "Saved Best Model\n",
      "Epoch [157/200], Loss: 2022.1563, Val Loss: 465.3790\n",
      "Saved Best Model\n",
      "Epoch [158/200], Loss: 2011.2743, Val Loss: 473.3133\n",
      "Epoch [159/200], Loss: 1994.5419, Val Loss: 450.8743\n",
      "Saved Best Model\n",
      "Epoch [160/200], Loss: 1961.5893, Val Loss: 437.7287\n",
      "Saved Best Model\n",
      "Epoch [161/200], Loss: 1950.5496, Val Loss: 626.2401\n",
      "Epoch [162/200], Loss: 1990.7225, Val Loss: 444.2526\n",
      "Epoch [163/200], Loss: 1923.6773, Val Loss: 429.4306\n",
      "Saved Best Model\n",
      "Epoch [164/200], Loss: 1882.9065, Val Loss: 404.2654\n",
      "Saved Best Model\n",
      "Epoch [165/200], Loss: 1850.8253, Val Loss: 394.5312\n",
      "Saved Best Model\n",
      "Epoch [166/200], Loss: 1830.0244, Val Loss: 392.3918\n",
      "Saved Best Model\n",
      "Epoch [167/200], Loss: 1816.6558, Val Loss: 379.7349\n",
      "Saved Best Model\n",
      "Epoch [168/200], Loss: 1787.7297, Val Loss: 365.4558\n",
      "Saved Best Model\n",
      "Epoch [169/200], Loss: 1766.4725, Val Loss: 355.7395\n",
      "Saved Best Model\n",
      "Epoch [170/200], Loss: 1772.3339, Val Loss: 380.1170\n",
      "Epoch [171/200], Loss: 1757.0077, Val Loss: 406.7114\n",
      "Epoch [172/200], Loss: 1730.2911, Val Loss: 342.6536\n",
      "Saved Best Model\n",
      "Epoch [173/200], Loss: 1697.3141, Val Loss: 330.6977\n",
      "Saved Best Model\n",
      "Epoch [174/200], Loss: 1672.1478, Val Loss: 334.7170\n",
      "Epoch [175/200], Loss: 1653.0251, Val Loss: 309.2798\n",
      "Saved Best Model\n",
      "Epoch [176/200], Loss: 1642.7494, Val Loss: 339.0802\n",
      "Epoch [177/200], Loss: 1698.2685, Val Loss: 369.9947\n",
      "Epoch [178/200], Loss: 1635.5145, Val Loss: 313.3326\n",
      "Epoch [179/200], Loss: 1585.9382, Val Loss: 289.6978\n",
      "Saved Best Model\n",
      "Epoch [180/200], Loss: 1565.4184, Val Loss: 279.6542\n",
      "Saved Best Model\n",
      "Epoch [181/200], Loss: 1546.4241, Val Loss: 282.9522\n",
      "Epoch [182/200], Loss: 1527.7994, Val Loss: 266.9812\n",
      "Saved Best Model\n",
      "Epoch [183/200], Loss: 1513.6092, Val Loss: 259.2552\n",
      "Saved Best Model\n",
      "Epoch [184/200], Loss: 1509.0942, Val Loss: 287.6422\n",
      "Epoch [185/200], Loss: 1486.8283, Val Loss: 252.4896\n",
      "Saved Best Model\n",
      "Epoch [186/200], Loss: 1461.4672, Val Loss: 240.0977\n",
      "Saved Best Model\n",
      "Epoch [187/200], Loss: 1448.8414, Val Loss: 234.6745\n",
      "Saved Best Model\n",
      "Epoch [188/200], Loss: 1428.8504, Val Loss: 229.5331\n",
      "Saved Best Model\n",
      "Epoch [189/200], Loss: 1412.1581, Val Loss: 233.3350\n",
      "Epoch [190/200], Loss: 1423.1727, Val Loss: 232.7956\n",
      "Epoch [191/200], Loss: 1409.4204, Val Loss: 238.4039\n",
      "Epoch [192/200], Loss: 1388.5118, Val Loss: 238.8917\n",
      "Epoch [193/200], Loss: 1355.3585, Val Loss: 204.6174\n",
      "Saved Best Model\n",
      "Epoch [194/200], Loss: 1333.0373, Val Loss: 201.1906\n",
      "Saved Best Model\n",
      "Epoch [195/200], Loss: 1315.7518, Val Loss: 192.6719\n",
      "Saved Best Model\n",
      "Epoch [196/200], Loss: 1299.6791, Val Loss: 189.5268\n",
      "Saved Best Model\n",
      "Epoch [197/200], Loss: 1291.2934, Val Loss: 198.8081\n",
      "Epoch [198/200], Loss: 1283.6205, Val Loss: 197.6666\n",
      "Epoch [199/200], Loss: 1269.0783, Val Loss: 195.1253\n",
      "Epoch [200/200], Loss: 1255.5262, Val Loss: 181.0253\n",
      "Saved Best Model\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 학습 설정\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Early Stopping을 위한 변수 추가\n",
    "best_val_loss = float('inf')\n",
    "patience = 6 # 조기 종료를 위한 patience 값 설정 (100 epoch 동안 개선이 없으면 종료)\n",
    "epochs_no_improve = 0 # 개선이 없었던 epoch 수를 기록하는 변수\n",
    "\n",
    "# # 텐서로 변환된 학습 데이터 (train_sequences_tensor, train_targets_tensor)를 DataLoader로 묶기\n",
    "# train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 학습 루프\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # 학습 모드 설정\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     for sequences, targets in train_loader:\n",
    "#         # 입력 데이터와 타겟을 GPU로 이동\n",
    "#         sequences = sequences.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         # 옵티마이저의 기울기 초기화\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # 모델의 예측값 계산\n",
    "#         outputs = model(sequences)\n",
    "\n",
    "#         # 손실 계산\n",
    "#         loss = criterion(outputs.squeeze(), targets)  # MSE 손실 함수\n",
    "#         loss.backward()  # 역전파로 기울기 계산\n",
    "#         optimizer.step()  # 옵티마이저로 가중치 갱신\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # 매 epoch마다 평균 손실 출력\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 검증 데이터셋 생성 (예시로 일부 데이터를 분리)\n",
    "# val_sequences_tensor, val_targets_tensor = train_sequences_tensor[:100], train_targets_tensor[:100]\n",
    "# train_sequences_tensor, train_targets_tensor = train_sequences_tensor[100:], train_targets_tensor[100:]\n",
    "\n",
    "# val_dataset = TensorDataset(val_sequences_tensor, val_targets_tensor)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 모델 저장을 위한 초기 설정\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_path = 'best_model.pth'\n",
    "\n",
    "# # 학습 및 검증 루프\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for sequences, targets in train_loader:\n",
    "#         sequences = sequences.to(device)\n",
    "#         targets = targets.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(sequences)\n",
    "#         loss = criterion(outputs.squeeze(), targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     # 검증 단계\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for sequences, targets in val_loader:\n",
    "#             sequences = sequences.to(device)\n",
    "#             targets = targets.to(device)\n",
    "#             outputs = model(sequences)\n",
    "#             loss = criterion(outputs.squeeze(), targets)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # 최적 모델 저장\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         torch.save(model.state_dict(), best_model_path)\n",
    "#         print(\"Saved Best Model\")\n",
    "\n",
    "\n",
    "# 텐서로 변환된 학습 데이터 (train_sequences_tensor, train_targets_tensor)를 DataLoader로 묶기\n",
    "train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 검증 데이터셋 크기 증가 (훈련 데이터의 20% 정도 추천)\n",
    "val_size = int(len(train_sequences_tensor) * 0.2)  # 훈련 데이터의 20%를 검증 데이터로 사용\n",
    "val_sequences_tensor, val_targets_tensor = train_sequences_tensor[:val_size], train_targets_tensor[:val_size]\n",
    "train_sequences_tensor, train_targets_tensor = train_sequences_tensor[val_size:], train_targets_tensor[val_size:]\n",
    "\n",
    "val_dataset = TensorDataset(val_sequences_tensor, val_targets_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 저장을 위한 초기 설정\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# 학습 및 검증 루프 + Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequences, targets in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping 구현\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(\"Saved Best Model\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f97c6a7-4967-4ac0-9528-9a29864b4307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 523.6329494714737\n",
      "Test RMSE: 18.58\n",
      "Test R² Score: 0.8002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9906/1158696436.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()  # 평가 모드로 전환 (드롭아웃 등을 비활성화)\n",
    "with torch.no_grad():\n",
    "    test_sequences_tensor = test_sequences_tensor.to(device)  # 테스트 데이터도 GPU로 이동\n",
    "    predicted_rul = model(test_sequences_tensor)\n",
    "\n",
    "# 모델이 예측한 RUL 값\n",
    "predicted_rul = predicted_rul.cpu().numpy().squeeze()  # GPU에서 예측한 값을 numpy로 변환\n",
    "\n",
    "total_score = evaluate_algorithm(true_rul, predicted_rul)\n",
    "rmse = np.sqrt(mean_squared_error(true_rul, predicted_rul))\n",
    "r2 = r2_score(true_rul, predicted_rul)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Test Score: {total_score}\")\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb37b69-4c6e-41a7-a4a4-db3b7702ff94",
   "metadata": {},
   "source": [
    "## 손실함수로 score function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6d78fa3-d04a-48a1-80a1-ea17957caadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 비대칭 스코어링 함수를 PyTorch 로 변환\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 비대칭 스코어링 함수\n",
    "def asymmetric_scoring(y_true, y_pred, a1=10, a2=13):\n",
    "    errors = y_pred - y_true\n",
    "    scores = torch.where(errors < 0, torch.exp(-errors / a1) - 1, torch.exp(errors / a2) - 1)\n",
    "    return torch.sum(scores)\n",
    "\n",
    "# 비대칭 스코어링 손실 함수\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, a1=10, a2=13):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.a1 = a1\n",
    "        self.a2 = a2\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return asymmetric_scoring(y_true, y_pred, self.a1, self.a2)\n",
    "\n",
    "# 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # RUL을 예측하기 위한 출력 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "811e7a87-c1db-4a9d-acc3-41f9736eda38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2363077547786.2402\n",
      "Epoch [2/50], Loss: 1461999608791.0400\n",
      "Epoch [3/50], Loss: 1116232281948.1599\n",
      "Epoch [4/50], Loss: 918693341921.2800\n",
      "Epoch [5/50], Loss: 768967108362.2400\n",
      "Epoch [6/50], Loss: 660711765852.1600\n",
      "Epoch [7/50], Loss: 576426639564.8000\n",
      "Epoch [8/50], Loss: 503785729802.2400\n",
      "Epoch [9/50], Loss: 449084302458.8800\n",
      "Epoch [10/50], Loss: 401665211760.6400\n",
      "Epoch [11/50], Loss: 361450786119.6800\n",
      "Epoch [12/50], Loss: 327860453007.3600\n",
      "Epoch [13/50], Loss: 299702597570.5600\n",
      "Epoch [14/50], Loss: 274530161131.5200\n",
      "Epoch [15/50], Loss: 252994051522.5600\n",
      "Epoch [16/50], Loss: 233641954631.6800\n",
      "Epoch [17/50], Loss: 216781140213.7600\n",
      "Epoch [18/50], Loss: 202506029260.8000\n",
      "Epoch [19/50], Loss: 187832853888.0000\n",
      "Epoch [20/50], Loss: 175959481589.7600\n",
      "Epoch [21/50], Loss: 165361752460.8000\n",
      "Epoch [22/50], Loss: 155305084108.8000\n",
      "Epoch [23/50], Loss: 146429620715.5200\n",
      "Epoch [24/50], Loss: 138139437219.8400\n",
      "Epoch [25/50], Loss: 130480354590.7200\n",
      "Epoch [26/50], Loss: 123581347061.7600\n",
      "Epoch [27/50], Loss: 117542832599.0400\n",
      "Epoch [28/50], Loss: 111456046571.5200\n",
      "Epoch [29/50], Loss: 106198117030.4000\n",
      "Epoch [30/50], Loss: 100971818721.2800\n",
      "Epoch [31/50], Loss: 96480753213.4400\n",
      "Epoch [32/50], Loss: 91978693048.3200\n",
      "Epoch [33/50], Loss: 88021754588.1600\n",
      "Epoch [34/50], Loss: 84190093066.2400\n",
      "Epoch [35/50], Loss: 80688566927.3600\n",
      "Epoch [36/50], Loss: 77341028700.1600\n",
      "Epoch [37/50], Loss: 74282646876.1600\n",
      "Epoch [38/50], Loss: 71210850979.8400\n",
      "Epoch [39/50], Loss: 68575354060.8000\n",
      "Epoch [40/50], Loss: 65935836528.6400\n",
      "Epoch [41/50], Loss: 63455935098.8800\n",
      "Epoch [42/50], Loss: 61223175936.0000\n",
      "Epoch [43/50], Loss: 58914119249.9200\n",
      "Epoch [44/50], Loss: 56982634283.5200\n",
      "Epoch [45/50], Loss: 54920523742.7200\n",
      "Epoch [46/50], Loss: 52995973550.0800\n",
      "Epoch [47/50], Loss: 51253947504.6400\n",
      "Epoch [48/50], Loss: 49626546411.5200\n",
      "Epoch [49/50], Loss: 47945079900.1600\n",
      "Epoch [50/50], Loss: 46442523507.2000\n"
     ]
    }
   ],
   "source": [
    "# 2. 학습 루프에서 비대칭 스코어링 함수 사용\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "# set_seed(100)\n",
    "\n",
    "input_size = train_sequences.shape[2]\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = AsymmetricLoss(a1=10, a2=13)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# 학습 설정\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# 텐서로 변환된 학습 데이터 (train_sequences_tensor, train_targets_tensor)를 DataLoader로 묶기\n",
    "train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for sequences, targets in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "129700ac-4682-4e1d-a86e-5d6a08802247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 177852.19478714466\n",
      "Test RMSE: 53.57\n",
      "Test R² Score: -0.6616\n"
     ]
    }
   ],
   "source": [
    "# 3. 평가 루프에서 비대칭 스코어링 함수 사용\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_algorithm(y_true_all, y_pred_all, a1=10, a2=13):\n",
    "    total_score = 0\n",
    "\n",
    "    for y_true, y_pred in zip(y_true_all, y_pred_all):\n",
    "        score = asymmetric_scoring(torch.tensor(y_true), torch.tensor(y_pred), a1, a2)\n",
    "        total_score += score.item()\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# 테스트 데이터셋 준비\n",
    "test_dataset = TensorDataset(test_sequences_tensor, test_targets_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "true_rul = []\n",
    "predicted_rul = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, targets in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(sequences)\n",
    "        true_rul.append(targets.cpu().numpy())\n",
    "        predicted_rul.append(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "# 리스트를 numpy 배열로 변환\n",
    "true_rul = np.concatenate(true_rul)\n",
    "predicted_rul = np.concatenate(predicted_rul)\n",
    "\n",
    "# 평가\n",
    "total_score = evaluate_algorithm(true_rul, predicted_rul, a1=10, a2=13)\n",
    "rmse = np.sqrt(mean_squared_error(true_rul, predicted_rul))\n",
    "r2 = r2_score(true_rul, predicted_rul)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Test Score: {total_score}\")\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f3051-3b85-4bf0-8838-2ed15c17686f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterLab (Poetry)",
   "language": "python",
   "name": "jupyter-lab-9mopgkzy-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
