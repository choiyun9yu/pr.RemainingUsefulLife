{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2888c954-a031-40ec-aa04-4fa179840a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define filepath to read dataset\n",
    "data_path = './dataset/'\n",
    "\n",
    "# define column names\n",
    "columns = ['engine_id', 'time_in_cycles'] + \\\n",
    "          ['operational_setting_1', 'operational_setting_2', 'operational_setting_3'] + \\\n",
    "          [f'sensor_measurement_{i}' for i in range(1, 22)]\n",
    "\n",
    "train_df = pd.read_csv((data_path+'train_FD001.txt'), sep=r'\\s+', header=None, names=columns)\n",
    "test_df = pd.read_csv((data_path+'test_FD001.txt'), sep=r'\\s+', header=None, names=columns)\n",
    "rul_test_df = pd.read_csv((data_path+'RUL_FD001.txt'), sep=r'\\s+', header=None, names=['RUL'])\n",
    "\n",
    "# Max cycle per engine (각 엔진의 마지막 cycle 을 고장 cycle 로 간주)\n",
    "max_cycle = train_df.groupby('engine_id')['time_in_cycles'].max()\n",
    "\n",
    "# Caculate RUL (최대 사이클 - 해당 칼럼 사이클)\n",
    "train_df = train_df.merge(max_cycle, on='engine_id', suffixes=('', '_max'))\n",
    "train_df['RUL'] = train_df['time_in_cycles_max'] - train_df['time_in_cycles']\n",
    "train_df.drop('time_in_cycles_max', axis=1, inplace=True)\n",
    "\n",
    "# 센서 값이 일정한 (변동이 없는) 컬럼을 제거\n",
    "sensor_columns = [f'sensor_measurement_{i}' for i in range(1, 22)]\n",
    "constant_sensors = train_df[sensor_columns].std(axis=0) == 0\n",
    "train_df.drop(columns=constant_sensors.index[constant_sensors], axis=1, inplace=True)\n",
    "test_df.drop(columns=constant_sensors.index[constant_sensors], axis=1, inplace=True)\n",
    "\n",
    "# Define the window size for moving average\n",
    "window_size = 3  # You can adjust this value\n",
    "\n",
    "# Function to apply moving average to sensor columns\n",
    "def apply_moving_average(df):\n",
    "    sensor_columns = [col for col in df.columns if 'sensor_measurement' in col]\n",
    "    \n",
    "    # Group by engine_id and apply moving average\n",
    "    df_grouped = df.groupby('engine_id')[sensor_columns]\n",
    "    \n",
    "    # Apply rolling mean\n",
    "    df[sensor_columns] = df_grouped.rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply moving average to train_df and test_df\n",
    "train_df = apply_moving_average(train_df)\n",
    "test_df = apply_moving_average(test_df)\n",
    "\n",
    "# Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "sensor_columns = [col for col in train_df.columns if 'sensor_measurement' in col]\n",
    "train_df[sensor_columns] = scaler.fit_transform(train_df[sensor_columns])\n",
    "test_df[sensor_columns] = scaler.transform(test_df[sensor_columns])\n",
    "\n",
    "# 시퀀스 길이 설정\n",
    "sequence_length = 50  # 50\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"주어진 데이터에서 시퀀스를 만드는 함수\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    engines = data['engine_id'].unique()\n",
    "    \n",
    "    for engine_id in engines:\n",
    "        engine_data = data[data['engine_id'] == engine_id]\n",
    "        engine_values = engine_data.drop(columns=['engine_id', 'RUL']).values\n",
    "        rul_values = engine_data['RUL'].values\n",
    "        \n",
    "        for i in range(len(engine_values) - sequence_length + 1):\n",
    "            sequences.append(engine_values[i: i + sequence_length])\n",
    "            targets.append(rul_values[i + sequence_length - 1])  # 시퀀스의 마지막 값이 목표 RUL\n",
    "            \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Train data에서 시퀀스와 타겟 생성\n",
    "train_sequences, train_targets = create_sequences(train_df, sequence_length)\n",
    "\n",
    "# 실제 RUL 값은 numpy 배열로 변환\n",
    "true_rul = rul_test_df.values.squeeze()  # RUL_FD001.csv에서 실제 RUL 값 (예: rul_test_df = pd.read_csv('RUL_FD001.csv'))\n",
    "\n",
    "def create_test_sequences(data, sequence_length):\n",
    "    \"\"\"테스트 데이터를 위해 마지막 sequence_length만큼 시퀀스를 만드는 함수\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    engines = data['engine_id'].unique()\n",
    "    \n",
    "    for engine_id in engines:\n",
    "        engine_data = data[data['engine_id'] == engine_id]\n",
    "        engine_values = engine_data.drop(columns=['engine_id']).values\n",
    "        \n",
    "        # 시퀀스 길이가 sequence_length보다 짧으면 앞에 0으로 패딩\n",
    "        if len(engine_values) < sequence_length:\n",
    "            padding = np.zeros((sequence_length - len(engine_values), engine_values.shape[1]))\n",
    "            engine_values = np.vstack((padding, engine_values))\n",
    "        \n",
    "        sequences.append(engine_values[-sequence_length:])  # 마지막 sequence_length 만큼 사용\n",
    "    \n",
    "    return np.array(sequences)\n",
    "\n",
    "# Test data에서 시퀀스 생성\n",
    "test_sequences = create_test_sequences(test_df, sequence_length)\n",
    "\n",
    "# 데이터를 텐서로 변환\n",
    "train_sequences_tensor = torch.tensor(train_sequences, dtype=torch.float32)\n",
    "train_targets_tensor = torch.tensor(train_targets, dtype=torch.float32)\n",
    "test_sequences_tensor = torch.tensor(test_sequences, dtype=torch.float32)\n",
    "test_targets_tensor = torch.tensor(true_rul, dtype=torch.float32)\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed=369):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # RUL을 예측하기 위한 출력 레이어\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM에 입력: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # hidden state 초기화\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # cell state 초기화\n",
    "        \n",
    "        # LSTM의 출력: (batch_size, sequence_length, hidden_size), (hn, cn)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 최종 타임스텝의 출력만 사용\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer를 통해 RUL 예측\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    # 최적의 가중치 저장하는 함수\n",
    "    def save_best_state_dict(self):\n",
    "        self.best_state_dict = self.state_dict()\n",
    "        return self.best_state_dict\n",
    "\n",
    "    # 최적의 가중치를 반환하는 함수\n",
    "    def get_best_state_dict(self):\n",
    "        return self.best_state_dict\n",
    "\n",
    "# 스코어 함수 선언 asymmetric_scoring 함수는 조기 예측과 늦은 예측에 대해 서로 다른 가중치를 적용하여 스코어를 계산한다. (조기 예측 가중치 a1, 늦은 예측 가중치 a2)\n",
    "def asymmetric_scoring(y_true, y_pred, a1=10, a2=13):\n",
    "    \"\"\"\n",
    "    비대칭 스코어링 함수\n",
    "    y_true: 실제 RUL 값 (numpy array)\n",
    "    y_pred: 예측된 RUL 값 (numpy array)\n",
    "    a1: 조기 예측에 대한 가중치\n",
    "    a2: 늦은 예측에 대한 가중치\n",
    "    \"\"\"\n",
    "    errors = y_pred - y_true\n",
    "    scores = np.where(errors < 0, np.exp(-errors / a1) - 1, np.exp(errors / a2) - 1)\n",
    "    return np.sum(scores)\n",
    "\n",
    "# evaluate_algorithm 함수는 여러 UUT 에 대해 총 스코어를 계산한다.\n",
    "def evaluate_algorithm(y_true_all, y_pred_all, a1=10, a2=13):\n",
    "    \"\"\"\n",
    "    알고리즘 평가 함수\n",
    "    y_true_all: 실제 RUL 값 리스트 (각 UUT별 numpy array)\n",
    "    y_pred_all: 예측된 RUL 값 리스트 (각 UUT별 numpy array)\n",
    "    a1: 조기 예측에 대한 가중치\n",
    "    a2: 늦은 예측에 대한 가중치\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "\n",
    "    for y_true, y_pred in zip(y_true_all, y_pred_all):\n",
    "        score = asymmetric_scoring(y_true, y_pred, a1, a2)\n",
    "        total_score += score\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb6b9d9-e90e-46f5-9bfd-b126743ea8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 9884.8270, Val Loss: 8922.8880\n",
      "Saved Best Model\n",
      "Epoch [2/200], Loss: 9047.0278, Val Loss: 8361.5311\n",
      "Saved Best Model\n",
      "Epoch [3/200], Loss: 8538.6224, Val Loss: 7887.4917\n",
      "Saved Best Model\n",
      "Epoch [4/200], Loss: 8086.4916, Val Loss: 7458.1187\n",
      "Saved Best Model\n",
      "Epoch [5/200], Loss: 7673.3734, Val Loss: 7061.5686\n",
      "Saved Best Model\n",
      "Epoch [6/200], Loss: 7292.3280, Val Loss: 6695.0668\n",
      "Saved Best Model\n",
      "Epoch [7/200], Loss: 6937.7239, Val Loss: 6353.6251\n",
      "Saved Best Model\n",
      "Epoch [8/200], Loss: 6608.7850, Val Loss: 6035.7595\n",
      "Saved Best Model\n",
      "Epoch [9/200], Loss: 6302.1702, Val Loss: 5742.6881\n",
      "Saved Best Model\n",
      "Epoch [10/200], Loss: 6017.6382, Val Loss: 5467.8375\n",
      "Saved Best Model\n",
      "Epoch [11/200], Loss: 5752.5210, Val Loss: 5213.4697\n",
      "Saved Best Model\n",
      "Epoch [12/200], Loss: 5507.4771, Val Loss: 4978.0144\n",
      "Saved Best Model\n",
      "Epoch [13/200], Loss: 5280.9361, Val Loss: 4760.1729\n",
      "Saved Best Model\n",
      "Epoch [14/200], Loss: 5070.6263, Val Loss: 4558.7296\n",
      "Saved Best Model\n",
      "Epoch [15/200], Loss: 4877.3769, Val Loss: 4374.3531\n",
      "Saved Best Model\n",
      "Epoch [16/200], Loss: 4701.2376, Val Loss: 4203.8006\n",
      "Saved Best Model\n",
      "Epoch [17/200], Loss: 4537.2363, Val Loss: 4048.8296\n",
      "Saved Best Model\n",
      "Epoch [18/200], Loss: 4386.5290, Val Loss: 3906.3610\n",
      "Saved Best Model\n",
      "Epoch [19/200], Loss: 4252.5951, Val Loss: 3777.3250\n",
      "Saved Best Model\n",
      "Epoch [20/200], Loss: 4128.2368, Val Loss: 3660.4468\n",
      "Saved Best Model\n",
      "Epoch [21/200], Loss: 4019.5506, Val Loss: 3555.1421\n",
      "Saved Best Model\n",
      "Epoch [22/200], Loss: 3915.4895, Val Loss: 3446.9004\n",
      "Saved Best Model\n",
      "Epoch [23/200], Loss: 3724.0460, Val Loss: 3183.9120\n",
      "Saved Best Model\n",
      "Epoch [24/200], Loss: 3574.7621, Val Loss: 3055.3067\n",
      "Saved Best Model\n",
      "Epoch [25/200], Loss: 3433.5125, Val Loss: 2859.7132\n",
      "Saved Best Model\n",
      "Epoch [26/200], Loss: 3187.6179, Val Loss: 2566.7966\n",
      "Saved Best Model\n",
      "Epoch [27/200], Loss: 2896.3283, Val Loss: 2377.0099\n",
      "Saved Best Model\n",
      "Epoch [28/200], Loss: 2710.2024, Val Loss: 2241.5427\n",
      "Saved Best Model\n",
      "Epoch [29/200], Loss: 2574.2815, Val Loss: 2086.4044\n",
      "Saved Best Model\n",
      "Epoch [30/200], Loss: 2417.4671, Val Loss: 1960.8271\n",
      "Saved Best Model\n",
      "Epoch [31/200], Loss: 2288.1186, Val Loss: 1871.1168\n",
      "Saved Best Model\n",
      "Epoch [32/200], Loss: 2181.3163, Val Loss: 1741.4886\n",
      "Saved Best Model\n",
      "Epoch [33/200], Loss: 2067.2681, Val Loss: 1653.1863\n",
      "Saved Best Model\n",
      "Epoch [34/200], Loss: 1976.2826, Val Loss: 1563.3188\n",
      "Saved Best Model\n",
      "Epoch [35/200], Loss: 1885.5167, Val Loss: 1487.7444\n",
      "Saved Best Model\n",
      "Epoch [36/200], Loss: 1793.6635, Val Loss: 1401.6409\n",
      "Saved Best Model\n",
      "Epoch [37/200], Loss: 1695.5951, Val Loss: 1324.9596\n",
      "Saved Best Model\n",
      "Epoch [38/200], Loss: 1613.7506, Val Loss: 1245.5091\n",
      "Saved Best Model\n",
      "Epoch [39/200], Loss: 1557.2124, Val Loss: 1191.1570\n",
      "Saved Best Model\n",
      "Epoch [40/200], Loss: 1483.2743, Val Loss: 1138.7728\n",
      "Saved Best Model\n",
      "Epoch [41/200], Loss: 1428.4317, Val Loss: 1066.9810\n",
      "Saved Best Model\n",
      "Epoch [42/200], Loss: 1356.0674, Val Loss: 1015.5486\n",
      "Saved Best Model\n",
      "Epoch [43/200], Loss: 1304.7649, Val Loss: 1005.8931\n",
      "Saved Best Model\n",
      "Epoch [44/200], Loss: 1254.7578, Val Loss: 932.9532\n",
      "Saved Best Model\n",
      "Epoch [45/200], Loss: 1204.0360, Val Loss: 906.7053\n",
      "Saved Best Model\n",
      "Epoch [46/200], Loss: 1158.6451, Val Loss: 853.6857\n",
      "Saved Best Model\n",
      "Epoch [47/200], Loss: 1118.3490, Val Loss: 814.8979\n",
      "Saved Best Model\n",
      "Epoch [48/200], Loss: 1088.5726, Val Loss: 789.9544\n",
      "Saved Best Model\n",
      "Epoch [49/200], Loss: 1057.0047, Val Loss: 779.1377\n",
      "Saved Best Model\n",
      "Epoch [50/200], Loss: 1005.0532, Val Loss: 721.0736\n",
      "Saved Best Model\n",
      "Epoch [51/200], Loss: 980.6475, Val Loss: 691.2746\n",
      "Saved Best Model\n",
      "Epoch [52/200], Loss: 951.2035, Val Loss: 694.6657\n",
      "Epoch [53/200], Loss: 922.2987, Val Loss: 639.2209\n",
      "Saved Best Model\n",
      "Epoch [54/200], Loss: 899.3672, Val Loss: 642.3578\n",
      "Epoch [55/200], Loss: 897.9918, Val Loss: 618.2285\n",
      "Saved Best Model\n",
      "Epoch [56/200], Loss: 856.7694, Val Loss: 604.3618\n",
      "Saved Best Model\n",
      "Epoch [57/200], Loss: 833.4668, Val Loss: 604.9140\n",
      "Epoch [58/200], Loss: 801.7729, Val Loss: 568.2517\n",
      "Saved Best Model\n",
      "Epoch [59/200], Loss: 797.3580, Val Loss: 604.2843\n",
      "Epoch [60/200], Loss: 775.2855, Val Loss: 538.5468\n",
      "Saved Best Model\n",
      "Epoch [61/200], Loss: 758.6498, Val Loss: 529.6805\n",
      "Saved Best Model\n",
      "Epoch [62/200], Loss: 747.1020, Val Loss: 529.3338\n",
      "Saved Best Model\n",
      "Epoch [63/200], Loss: 735.4441, Val Loss: 519.5460\n",
      "Saved Best Model\n",
      "Epoch [64/200], Loss: 722.1455, Val Loss: 540.8269\n",
      "Epoch [65/200], Loss: 733.7947, Val Loss: 498.6353\n",
      "Saved Best Model\n",
      "Epoch [66/200], Loss: 702.3267, Val Loss: 530.6048\n",
      "Epoch [67/200], Loss: 691.0365, Val Loss: 565.5990\n",
      "Epoch [68/200], Loss: 695.1082, Val Loss: 457.9972\n",
      "Saved Best Model\n",
      "Epoch [69/200], Loss: 658.4806, Val Loss: 507.5578\n",
      "Epoch [70/200], Loss: 660.7876, Val Loss: 471.9871\n",
      "Epoch [71/200], Loss: 656.8428, Val Loss: 449.1149\n",
      "Saved Best Model\n",
      "Epoch [72/200], Loss: 633.9439, Val Loss: 513.7576\n",
      "Epoch [73/200], Loss: 631.2677, Val Loss: 486.3996\n",
      "Epoch [74/200], Loss: 632.5586, Val Loss: 536.3666\n",
      "Epoch [75/200], Loss: 618.0039, Val Loss: 453.0985\n",
      "Epoch [76/200], Loss: 621.1591, Val Loss: 415.5946\n",
      "Saved Best Model\n",
      "Epoch [77/200], Loss: 598.4509, Val Loss: 433.6793\n",
      "Epoch [78/200], Loss: 605.4848, Val Loss: 457.3521\n",
      "Epoch [79/200], Loss: 601.3282, Val Loss: 394.9991\n",
      "Saved Best Model\n",
      "Epoch [80/200], Loss: 597.0801, Val Loss: 415.3177\n",
      "Epoch [81/200], Loss: 619.7174, Val Loss: 419.2604\n",
      "Epoch [82/200], Loss: 576.2114, Val Loss: 409.4973\n",
      "Epoch [83/200], Loss: 576.7197, Val Loss: 415.9773\n",
      "Epoch [84/200], Loss: 575.8803, Val Loss: 425.2358\n",
      "Epoch [85/200], Loss: 572.2078, Val Loss: 387.1990\n",
      "Saved Best Model\n",
      "Epoch [86/200], Loss: 571.8165, Val Loss: 391.7668\n",
      "Epoch [87/200], Loss: 567.3356, Val Loss: 475.7035\n",
      "Epoch [88/200], Loss: 612.4057, Val Loss: 436.7177\n",
      "Epoch [89/200], Loss: 584.9029, Val Loss: 403.2453\n",
      "Epoch [90/200], Loss: 568.1156, Val Loss: 383.8200\n",
      "Saved Best Model\n",
      "Epoch [91/200], Loss: 557.8954, Val Loss: 411.2141\n",
      "Epoch [92/200], Loss: 559.7220, Val Loss: 464.5311\n",
      "Epoch [93/200], Loss: 552.9798, Val Loss: 430.2965\n",
      "Epoch [94/200], Loss: 540.1644, Val Loss: 440.8249\n",
      "Epoch [95/200], Loss: 545.7271, Val Loss: 389.0298\n",
      "Epoch [96/200], Loss: 553.6503, Val Loss: 403.8062\n",
      "Early stopping triggered after 96 epochs.\n",
      "Test Score: 1193.5578769313897\n",
      "Test RMSE: 19.99\n",
      "Test R² Score: 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10910/4132973108.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "input_size = train_sequences.shape[2]  # 센서/피처 수\n",
    "hidden_size = 50  # LSTM hidden state 크기 (높을 수로 모델 복잡해짐)\n",
    "num_layers = 2  # LSTM 레이어 수\n",
    "learning_rate = 0.0005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, hidden_size, num_layers).to(device)  # 모델을 GPU로 이동\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "set_seed(369)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.MSELoss()  # RUL 예측이므로 MSELoss 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam Optimizer 사용\n",
    "\n",
    "# 학습 설정\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Early Stopping을 위한 변수 추가\n",
    "best_val_loss = float('inf')\n",
    "patience = 6 # 조기 종료를 위한 patience 값 설정 (100 epoch 동안 개선이 없으면 종료)\n",
    "epochs_no_improve = 0 # 개선이 없었던 epoch 수를 기록하는 변수\n",
    "\n",
    "# 텐서로 변환된 학습 데이터 (train_sequences_tensor, train_targets_tensor)를 DataLoader로 묶기\n",
    "train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 검증 데이터셋 크기 증가 (훈련 데이터의 20% 정도 추천)\n",
    "val_size = int(len(train_sequences_tensor) * 0.2)  # 훈련 데이터의 20%를 검증 데이터로 사용\n",
    "val_sequences_tensor, val_targets_tensor = train_sequences_tensor[:val_size], train_targets_tensor[:val_size]\n",
    "train_sequences_tensor, train_targets_tensor = train_sequences_tensor[val_size:], train_targets_tensor[val_size:]\n",
    "\n",
    "val_dataset = TensorDataset(val_sequences_tensor, val_targets_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 저장을 위한 초기 설정\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# 학습 및 검증 루프 + Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequences, targets in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping 구현\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(\"Saved Best Model\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()  # 평가 모드로 전환 (드롭아웃 등을 비활성화)\n",
    "with torch.no_grad():\n",
    "    test_sequences_tensor = test_sequences_tensor.to(device)  # 테스트 데이터도 GPU로 이동\n",
    "    predicted_rul = model(test_sequences_tensor)\n",
    "\n",
    "# 모델이 예측한 RUL 값\n",
    "predicted_rul = predicted_rul.cpu().numpy().squeeze()  # GPU에서 예측한 값을 numpy로 변환\n",
    "\n",
    "total_score = evaluate_algorithm(true_rul, predicted_rul)\n",
    "rmse = np.sqrt(mean_squared_error(true_rul, predicted_rul))\n",
    "r2 = r2_score(true_rul, predicted_rul)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Test Score: {total_score}\")\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efe0be-e62c-4ab3-b9ae-b6e0718cf955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterLab (Poetry)",
   "language": "python",
   "name": "jupyter-lab-9mopgkzy-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
